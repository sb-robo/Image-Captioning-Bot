{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Read all necessary files that we have save for future using purpose`\n",
    "\n",
    "- caption_dict -> Caption_description.pkl\n",
    "- train_caption -> train_description.pkl\n",
    "- word_to_idx -> Word_to_idx.pkl\n",
    "- idx_to_word -> Idx_to_word.pkl\n",
    "- emb_mat -> embedding_matrix.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Caption_description.pkl', 'rb') as f:\n",
    "    caption_dict = pickle.load(f)\n",
    "    \n",
    "with open('./train_description.pkl', 'rb') as f:\n",
    "    train_caption = pickle.load(f)\n",
    "    \n",
    "with open('./Word_to_idx.pkl','rb') as f:\n",
    "    word_to_idx = pickle.load(f)\n",
    "    \n",
    "with open('./Idx_to_word.pkl','rb') as f:\n",
    "    idx_to_word = pickle.load(f)\n",
    "    \n",
    "with open('./embedding_matrix.npy','rb') as f:\n",
    "    emb_mat = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2968, 50)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "# Find max length caption\n",
    "\n",
    "max_length = 0\n",
    "for img_name in train_caption.keys():\n",
    "    for caption in train_caption[img_name]:\n",
    "        \n",
    "        max_length = max(max_length,len(caption.split()))\n",
    "        \n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2967\n"
     ]
    }
   ],
   "source": [
    "vocab_size =len(word_to_idx)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import extracted training images features\n",
    "\n",
    "with open('./Encoded features/encoded_train_feature.pkl','rb') as f:\n",
    "    encoding_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator\n",
    "\n",
    "- This function will predict next word from past sequence of words\n",
    "    - Example\n",
    "        - This\n",
    "        - This is\n",
    "        - This is a\n",
    "        - This is a ball\n",
    "        \n",
    "From the above example we can understand that what our model should predict\n",
    "\n",
    "`Note:` We will not pass word, inspite we will pass interger value which will represnt word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(train_caption,encoding_train,word_to_idx,batch_size,max_length):\n",
    "    x1,x2,y = [],[],[]        #here we will pass photo, word sequence, output\n",
    "    \n",
    "    n=0\n",
    "    while True:\n",
    "        for key,caption_list in train_caption.items():\n",
    "            n += 1\n",
    "            encoded_img = encoding_train[key]       #find images encoeded features\n",
    "            for caption in caption_list:\n",
    "                seq = [word_to_idx[word] for word in caption.split() if word in word_to_idx.keys()]\n",
    "                \n",
    "                for i in range(0,len(seq)):\n",
    "                    xi = seq[0:i]\n",
    "                    yi = seq[i]\n",
    "                    \n",
    "                    xi = pad_sequences([xi],maxlen=max_length,value=0,padding='post')[0]\n",
    "                    #we are using padding after x2 to make same length\n",
    "                    \n",
    "                    yi = to_categorical([yi],num_classes=vocab_size+1)[0]   \n",
    "                    # neural networks accept on-hot vector\n",
    "                    \n",
    "                    x1.append(encoded_img)\n",
    "                    x2.append(xi)\n",
    "                    y.append(yi)\n",
    "                    \n",
    "                    \n",
    "                if n==batch_size:\n",
    "                    \n",
    "                    x1 = np.asarray(x1)\n",
    "                    x2 = np.asarray(x2)\n",
    "                    y = np.asarray(y)\n",
    "                    \n",
    "                    \"\"\"x1 = np.asarray(x1).astype(np.float32)\n",
    "                    x2 = np.asarray(x2).astype(np.float32)\n",
    "                    y = np.asarray(y)#.astype(np.float32)\"\"\"\n",
    "                    \n",
    "                    yield ([x1,x2],y)    #pass this values\n",
    "                    x1,x2,y=[],[],[]  # we do not want to add examples which are there in previous batch \n",
    "                    n=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`The Model Architecture is not a Sequential model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To handle Images\n",
    "img_input = Input(shape=(2048,))     #sahpe of extracted features of images, is taken as input\n",
    "img_inp1 = Dropout(0.3)(img_input)\n",
    "img_inp2 = Dense(256,activation='relu')(img_inp1)   #coverting input dim(2048) to 256dim\n",
    "\n",
    "# To handle Cptions\n",
    "caption_input = Input(shape=(max_length,))          #Length of the vocab is the input shape o captions\n",
    "cap_inp1 = Embedding(input_dim=vocab_size+1,output_dim=50,mask_zero=True)(caption_input)\n",
    "cap_inp2 = Dropout(0.3)(cap_inp1)\n",
    "cap_inp3 = LSTM(256)(cap_inp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 33)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 2048)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 33, 50)       97950       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 2048)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 33, 50)       0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          524544      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 256)          314368      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 256)          0           dense[0][0]                      \n",
      "                                                                 lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1959)         503463      dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,506,117\n",
      "Trainable params: 1,506,117\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# main Model\n",
    "# Here we have to make a decoder which will take input from two concatenated vectors\n",
    "\n",
    "decoder1 = add([img_inp2,cap_inp3]) #this layer will take pne input from the image and the other is from the captions\n",
    "decoder2 = Dense(256,activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size+1,activation='softmax')(decoder2)\n",
    "\n",
    "#  Combine Model\n",
    "model = Model([img_input,caption_input],outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set embedding layer weights\n",
    "\n",
    "model.layers[2].set_weights([emb_mat])   #we are setting embedding layers weight which we get from embedding matrix\n",
    "model.layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory exists\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "batch_size = 3\n",
    "epochs = 10\n",
    "images_per_batch = 3\n",
    "steps = len(train_caption)//images_per_batch\n",
    "\n",
    "try:\n",
    "    os.mkdir('./model_weights')\n",
    "    print('Directory Created')\n",
    "except:\n",
    "    print('Directory exists')\n",
    "    files = glob.glob('./model_weights/*')\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "    \n",
    "    \n",
    "def train():\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        generator = data_generator(train_caption,encoding_train,word_to_idx,batch_size,max_length)\n",
    "        h = model.fit(generator,epochs=1,steps_per_epoch=steps,verbose=1)\n",
    "        model.save(f'./model_weights/model_{i+1}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 492s 246ms/step - loss: 4.6806\n",
      "2000/2000 [==============================] - 559s 280ms/step - loss: 3.9776\n",
      "2000/2000 [==============================] - 591s 295ms/step - loss: 3.7166\n",
      "2000/2000 [==============================] - 591s 296ms/step - loss: 3.5455\n",
      "2000/2000 [==============================] - 533s 267ms/step - loss: 3.4248\n",
      "2000/2000 [==============================] - 506s 253ms/step - loss: 3.3294\n",
      "2000/2000 [==============================] - 489s 244ms/step - loss: 3.2562\n",
      "2000/2000 [==============================] - 488s 244ms/step - loss: 3.1917\n",
      "2000/2000 [==============================] - 488s 244ms/step - loss: 3.1389\n",
      "2000/2000 [==============================] - 490s 245ms/step - loss: 3.0948\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}