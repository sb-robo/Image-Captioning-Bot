{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Read all necessary files that we have save for future using purpose`\n",
    "\n",
    "- caption_dict -> Caption_description.pkl\n",
    "- train_caption -> train_description.pkl\n",
    "- word_to_idx -> Word_to_idx.pkl\n",
    "- idx_to_word -> Idx_to_word.pkl\n",
    "- emb_mat -> embedding_matrix.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Caption_description.pkl', 'rb') as f:\n",
    "    caption_dict = pickle.load(f)\n",
    "    \n",
    "with open('./train_description.pkl', 'rb') as f:\n",
    "    train_caption = pickle.load(f)\n",
    "    \n",
    "with open('./Word_to_idx.pkl','rb') as f:\n",
    "    word_to_idx = pickle.load(f)\n",
    "    \n",
    "with open('./Idx_to_word.pkl','rb') as f:\n",
    "    idx_to_word = pickle.load(f)\n",
    "    \n",
    "with open('./embedding_matrix.npy','rb') as f:\n",
    "    emb_mat = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1959, 50)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "# Find max length caption\n",
    "\n",
    "max_length = 0\n",
    "for img_name in train_caption.keys():\n",
    "    for caption in train_caption[img_name]:\n",
    "        \n",
    "        max_length = max(max_length,len(caption.split()))\n",
    "        \n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1958\n"
     ]
    }
   ],
   "source": [
    "vocab_size =len(word_to_idx)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import extracted training images features\n",
    "\n",
    "with open('./Encoded features/encoded_train_feature.pkl','rb') as f:\n",
    "    encoding_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator\n",
    "\n",
    "- This function will predict next word from past sequence of words\n",
    "    - Example\n",
    "        - This\n",
    "        - This is\n",
    "        - This is a\n",
    "        - This is a ball\n",
    "        \n",
    "From the above example we can understand that what our model should predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(train_caption,encoding_train,word_to_idx,batch_size,max_length):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`The Model Architecture is not a Sequential model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To handle Images\n",
    "img_input = Input(shape=(2048,))     #sahpe of extracted features of images, is taken as input\n",
    "img_inp1 = Dropout(0.3)(img_input)\n",
    "img_inp2 = Dense(256,activation='relu')(img_inp1)   #coverting input dim(2048) to 256dim\n",
    "\n",
    "# To handle Cptions\n",
    "caption_input = Input(shape=(max_length,))          #Length of the vocab is the input shape o captions\n",
    "cap_inp1 = Embedding(input_dim=vocab_size+1,output_dim=50,mask_zero=True)(caption_input)\n",
    "cap_inp2 = Dropout(0.3)(cap_inp1)\n",
    "cap_inp3 = LSTM(256)(cap_inp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 33)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 2048)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 33, 50)       97950       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 2048)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 33, 50)       0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          524544      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 256)          314368      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 256)          0           dense[0][0]                      \n",
      "                                                                 lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1959)         503463      dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,506,117\n",
      "Trainable params: 1,506,117\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# main Model\n",
    "# Here we have to make a decoder which will take input from two concatenated vectors\n",
    "\n",
    "decoder1 = add([img_inp2,cap_inp3]) #this layer will take pne input from the image and the other is from the captions\n",
    "decoder2 = Dense(256,activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size+1,activation='softmax')(decoder2)\n",
    "\n",
    "#  Combine Model\n",
    "model = Model([img_input,caption_input],outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set embedding layer weights\n",
    "\n",
    "model.layers[2].set_weights([emb_mat])   #we are setting embedding layers weight which we get from embedding matrix\n",
    "model.layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
